{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSCI 303\n",
    "# Introduction to Data Science\n",
    "<p/>\n",
    "\n",
    "## (Decision Trees and) Random Forests\n",
    "\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S0031320310003973-gr1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "- Train an \"**ensemble**\" of decisions trees\n",
    "- RF decision is\n",
    " - the mean of decisions by individual trees (regression)\n",
    " - the majority vote of decisions by individual trees (classification)\n",
    " \n",
    "**Pros**\n",
    "- Reduces variance of individual decision tree models\n",
    "- Can rank the importance of features in a natural way\n",
    "- Can operate on many (1000s) of features without dimensionality reduction\n",
    "- Can train well with relatively few samples\n",
    "\n",
    "**Cons**\n",
    "- Loss of interpretability, compared to individual decision trees\n",
    "- Not as good at regression:\n",
    " - Output is not truly continuous valued, but rather, discretized (less problematic than for individual trees, however)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest trees are different from regular decision trees\n",
    "\n",
    "**If the same training set and method is used for each tree, all the trees will be correlated**  \n",
    "(produce nearly identical outputs) and nothing will be gained by having an ensemble.\n",
    "\n",
    "Use two forms of **randomization** when training each tree, to **reduce tree correlation**.\n",
    "1. **Bagging**\n",
    "2. **Feature randomization** (**feature bagging**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging (bootstrap aggregating)\n",
    "\n",
    "The training set for each tree is B samples randomly drawn from the full training set, **with replacement**.\n",
    "\n",
    "![Bagging](https://miro.medium.com/max/1280/1*Wf91XObaX2zwow7mMwDmGw.png)\n",
    "\n",
    "**Bonus**: We can get an **\"out-of-bag\" (OOB)** score from our training set:  \n",
    "For a given sample, we make a prediction using subset of trees that were not trained with that sample.\n",
    "This is a from of cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature subset randomization\n",
    "\n",
    "During training of **individual RF trees**, a random subset of **features** is selected at each branch split in the tree. In regular decision trees, all features are considered.\n",
    "\n",
    "**The figure below doesn't quite do this justice.** It shows a subset of features being using for each tree.\n",
    "\n",
    "However, in RF training of a single tree, **a random subset of features is selected at each split,  \n",
    "and from those features one is then selected as the feature on which the split the data** (based on  \n",
    "Gini impurity or some other metric).\n",
    "![Feature bagging](https://i.ytimg.com/vi/goPiwckWE9M/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll work with the Titanic data set again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get necessary packages\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz # needed to visualize trained decision tree\n",
    "from sklearn.tree import export_graphviz # needed to visualize trained decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Set up for plotting\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Read in our data and glance at its formatting\n",
    "df = pd.read_csv('titanic_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare our data for viable input to the sklearn decision tree model\n",
    "\n",
    "# Convert Sex feature into a pair of Boolean/binary dummy features (male and female)\n",
    "df_dummy = pd.get_dummies(df, columns=['Sex'])\n",
    "\n",
    "# Remove redundant male or female feature, and unneeded Name feature\n",
    "df_dummy = df_dummy.drop(columns=['Sex_male', 'Name'])\n",
    "df_dummy.head()\n",
    "\n",
    "# Create train/test data sets\n",
    "X = df_dummy.drop(columns=['Survived'])\n",
    "y = df_dummy['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's repeat our work on decision trees, for eventual comparison with random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's train models over a range of depths, and score them with the test set\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "acc_scores_dt = []\n",
    "f1_scores_dt = []\n",
    "max_max = 20\n",
    "max_depth = np.arange(1, max_max)\n",
    "\n",
    "for depth in max_depth:\n",
    "    # Build model and train\n",
    "    titanic_tree = DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "    titanic_tree.fit(X_train, y_train)\n",
    "\n",
    "    # Test\n",
    "    acc_scores_dt.append(titanic_tree.score(X_test, y_test))\n",
    "    y_test_hat = titanic_tree.predict(X_test)\n",
    "    f1_scores_dt.append(f1_score(y_test, y_test_hat))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(max_depth, acc_scores_dt, 'ro-', label='Accuracy')\n",
    "plt.plot(max_depth, f1_scores_dt, 'bo-', label='F1')\n",
    "plt.legend()\n",
    "\n",
    "# Print values for best test score\n",
    "ix_best = np.argmax(acc_scores_dt)\n",
    "print('Best accuracy score is %0.3f, for max_depth=%d' % (acc_scores_dt[ix_best], max_depth[ix_best]))\n",
    "ix_best = np.argmax(f1_scores_dt)\n",
    "print('Best F1-score is %0.3f, for max_depth=%d' % (f1_scores_dt[ix_best], max_depth[ix_best]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll use sklearn to train random forests on the same data set\n",
    "\n",
    "#### In addition to scores for the test set, we'll measure the _out-of-bag_ error from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## We'll train models over a range of tree depths\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "n_estimators = 100\n",
    "max_depth = np.arange(1, max_max)\n",
    "acc_scores_rf = []\n",
    "f1_scores_rf = []\n",
    "oob_scores_rf = []\n",
    "for depth in max_depth:\n",
    "    # Build model and train\n",
    "    titanic_forest = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                            max_depth=depth,\n",
    "                                            oob_score=True,\n",
    "                                            random_state=0) # call the RandomForestClassifier on our parameters to build the model\n",
    "    titanic_forest.fit(X_train, y_train)\n",
    "    \n",
    "    # Save out-of-bag (OOB) score\n",
    "    oob_scores_rf.append(titanic_forest.oob_score_) \n",
    "    \n",
    "    # Test\n",
    "    acc_scores_rf.append(titanic_forest.score(X_test, y_test))\n",
    "    y_test_hat = titanic_forest.predict(X_test)\n",
    "    f1_scores_rf.append(f1_score(y_test, y_test_hat))\n",
    "    \n",
    "# Print values for best test score\n",
    "ix_best = np.argmax(acc_scores_rf)\n",
    "print('Best accuracy score is %0.3f, for max_depth=%d' % (acc_scores_rf[ix_best], max_depth[ix_best]))\n",
    "ix_best = np.argmax(f1_scores_rf)\n",
    "print('Best F1-score is %0.3f, for max_depth=%d' % (f1_scores_rf[ix_best], max_depth[ix_best]))\n",
    "ix_best = np.argmax(oob_scores_rf)\n",
    "print('Best OOB score is %0.3f, for max_depth=%d' % (oob_scores_rf[ix_best], max_depth[ix_best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(max_depth, acc_scores_rf, 'ro-', label='RF: Accuracy')\n",
    "plt.plot(max_depth, f1_scores_rf, 'bo-', label='RF: F1')\n",
    "plt.plot(max_depth, oob_scores_rf, 'go-', label='RF: OOB')\n",
    "plt.plot(max_depth, acc_scores_dt, 'ro--', alpha=0.3, label='DT: Accuracy')\n",
    "plt.plot(max_depth, f1_scores_dt, 'bo--', alpha=0.3, label='DT: F1')\n",
    "plt.xlabel('Maximum tree depth')\n",
    "plt.ylabel('Score')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see which features have the highest \"importance\" for an RF with the best max_depth.\n",
    "\n",
    "Importance, in sklearn, is calculated as the **Mean Decrease in Impurity (MDI)**.  \n",
    "That is, the **averge decrease in Gini impurity** across nodes that use the feature for splitting, weighted by the number of samples that reach that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_best = np.argmax(acc_scores_rf)\n",
    "max_depth_best = max_depth[ix_best]\n",
    "\n",
    "# Build model and train\n",
    "titanic_forest = RandomForestClassifier(n_estimators=n_estimators, random_state=0, max_depth=max_depth_best)\n",
    "titanic_forest.fit(X_train, y_train)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "column_names = X_train.columns\n",
    "x_tick = np.array(range(len(column_names)))\n",
    "plt.bar(x_tick, titanic_forest.feature_importances_)\n",
    "plt.xticks(x_tick, column_names, rotation=90, fontsize=18)\n",
    "_ = plt.ylabel('Importance', fontsize=18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
